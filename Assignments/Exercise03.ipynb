{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, the implementation is divided in four groups labeled as TM1, TM2, TM3 and TM4. Each team member will work in only one of the groups, however, all groups should be complete in order for the algorithm to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we will implement logistic regression using batch gradient descent. The next steps will guide you through the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we will load the numpy and matplotlib library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the dataset that we will use for training and testing our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = np.load('./data/ex03.reg.train.ft.npy') #training features\n",
    "ys = np.load('./data/ex03.reg.train.lb.npy') #training labels\n",
    "\n",
    "Xt = np.load('./data/ex03.reg.test.ft.npy') #testing features\n",
    "yt = np.load('./data/ex03.reg.test.lb.npy') #testing labels\n",
    "\n",
    "thi = np.load('./data/ex03.reg.w.npy') #initial parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features Matrix (Xs and Xt) are $m\\times n$ numpy arrays where each row correspond to a feature vector and the labels (ys and yt) are $m\\times 1$ numpy arrays where each element can be either 0 or 1. Let's print the first 5 elements, to see how they look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Features Matrix\n",
      "Shape: (400, 2)\n",
      "First 5 elements:\n",
      "[[ 0.0257384   0.07356659]\n",
      " [ 0.05684302  0.0531371 ]\n",
      " [ 0.0428545   0.06111346]\n",
      " [ 0.10163394  0.09047114]\n",
      " [ 0.03650691  0.03184156]]\n",
      "*Labels\n",
      "Shape: (400, 1)\n",
      "First 5 elements:\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "*Initial weights\n",
      "(3, 1)\n",
      "[[ -4.24165502]\n",
      " [ 26.7368333 ]\n",
      " [ 29.77781363]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('*Features Matrix')\n",
    "print('Shape: ' + str(Xs.shape))\n",
    "print('First 5 elements:\\n' + str(Xs[0:5]))\n",
    "print('*Labels')\n",
    "print('Shape: ' +  str(ys.shape))\n",
    "print('First 5 elements:\\n' + str(ys[0:5]))\n",
    "print('*Initial weights')\n",
    "print(thi.shape)\n",
    "print(thi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $m$ is the number of samples and $n$ the number of features. We have $m=400$ for the training set and $m=200$ for the testing set. The number of features is $n=2$. We can obtain this values using the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.0257384 ,  0.07356659],\n",
       "       [ 1.        ,  0.05684302,  0.0531371 ],\n",
       "       [ 1.        ,  0.0428545 ,  0.06111346]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Xs.shape[0]\n",
    "n = Xs.shape[1]\n",
    "Xs =  np.concatenate((np.ones((Xs.shape[0],1)),Xs),axis = 1)\n",
    "Xt =  np.concatenate((np.ones((Xt.shape[0],1)),Xt),axis = 1)\n",
    "Xs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM1$. Now let's begin with the implementation. As first point, we will implement a function for evaluating the sigmoid function given a set of parameters $g(\\theta^t {\\bf x}) = g(z) = 1 / (1 + exp(-z))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the function will be:\n",
    "xs.- A numpy array of dimension $m \\times n$ with the features Matrix.\n",
    "th.- Is the vector of parameters or weights of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should return a $m \\times 1$ numpy array $h$ where each element $h[i, 0] = g(\\theta^t {\\bf x^{(i)}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(xs, th):\n",
    "    return 1.0/(1.0 + np.exp(-(np.dot(xs,th))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the function by running the next code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.0257384   0.07356659]\n",
      " [ 1.          0.05684302  0.0531371 ]\n",
      " [ 1.          0.0428545   0.06111346]]\n"
     ]
    }
   ],
   "source": [
    "Xaux = Xs[0:3]\n",
    "gzaux = eval(xs=Xaux, th=thi)\n",
    "print(Xaux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.0257384 ,  0.07356659],\n",
       "       [ 1.        ,  0.05684302,  0.0531371 ],\n",
       "       [ 1.        ,  0.0428545 ,  0.06111346]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = eval(xs= Xs, th = thi)\n",
    "Xaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an array similar to this: <br>\n",
    "[ [ 0.0257384 &nbsp;  0.07356659]<br>\n",
    " [ 0.05684302 &nbsp; 0.0531371 ]<br>\n",
    " [ 0.0428545 &nbsp;  0.06111346] ]<br>\n",
    " Maybe with some decimal variations due to possible differences in precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM2$. Implement the loss function. Now, the parameters are: \n",
    "xs.- A numpy array of dimension $m \\times n$ with the features Matrix.\n",
    "ys.- A numpy array of dimensions $m \\times 1$ with the labels.\n",
    "th.- The vector of parameters o weights of the model. \n",
    "lamb.- The regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should return a single float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(xs, ys, th, lamb=0):\n",
    "    \n",
    "    h = eval(xs= xs, th = th)\n",
    "    total_loss = -(np.sum(ys*np.log(h)) + np.sum((1-ys)*np.log(1-h))) + 0.5*lamb*np.sum(th**2)\n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check we can run the next code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.348207023\n",
      "912.132245568\n"
     ]
    }
   ],
   "source": [
    "laux = loss(xs=Xs, ys=ys, th=thi)\n",
    "print(laux)\n",
    "laux = loss(xs=Xs, ys=ys, th=thi, lamb=1.0)\n",
    "print(laux)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get values around 102.3482 and 903.1364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM3$. Implement a function for compute the gradient as $G_j = \\sum_{i=1,\\dots,m}[y^{(i)}-h_\\theta(x^{(i)})]x^{(i)}_j$. The parameters for this function are the same than the last function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should return a $n \\times 1$ numpy array with the gradients for each of the $n$ weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient(xs, ys, th, lamb=0):  \n",
    "    gradient = -np.dot(xs.T,ys-eval(xs,th)) + lamb*th \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the gradient for the current state of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25212338]\n",
      " [-1.60139881]\n",
      " [-1.75284086]]\n",
      "[[ -3.98953164]\n",
      " [ 25.1354345 ]\n",
      " [ 28.02497277]]\n"
     ]
    }
   ],
   "source": [
    "gaux = batch_gradient(xs=Xs, ys=ys, th=thi)\n",
    "print(gaux)\n",
    "gaux = batch_gradient(xs=Xs, ys=ys, th=thi, lamb=1.0)\n",
    "print(gaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get two arrays with values around:<br>\n",
    "[ [-0.25212338]<br>\n",
    " [ 1.60139881]<br>\n",
    " [ 1.75284086] ]<br>\n",
    "[ [ -0.25212338]<br>\n",
    " [-25.1354345 ]<br>\n",
    " [-28.02497277] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM4$. Implement the gradient descent algorithm. The parameters for this function are: \n",
    "xs.- A numpy array of dimension $m \\times n$ with the features Matrix.\n",
    "ys.- A numpy array of dimensions $m \\times 1$ with the labels.\n",
    "th_init.- The initial vector of parameters o weights of the model. \n",
    "lt.- A float with the learning rate.\n",
    "T.- The number of running iterations.\n",
    "lamb.- The regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the optimized vector of parameters (th) and a python array with the loss for each iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def train(xs, ys, th_init, lr, T, lamb=0):\n",
    "    losses = []\n",
    "    th = np.copy(th_init)\n",
    "    for i in range(T):\n",
    "        losses.append(loss(xs, ys, th, lamb))\n",
    "        #---TM4.- Compute the gradient and update the weitghs here:\n",
    "        gradient = batch_gradient(xs= xs, ys= ys, th = th, lamb = lamb)\n",
    "        th -= lr*gradient \n",
    "        sys.stdout.write('\\r Training Progress : ( %d / %d)   | loss : %0.4f' %(i,T,losses[-1]))           \n",
    "        sys.stdout.flush()\n",
    "        #-----------------------------------------------\n",
    "    losses.append(loss(xs, ys, th, lamb))\n",
    "    return th, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for testing, we set up the hyperparameters and load a $n \\times 1$ numpy array with the initial weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "T = 10000\n",
    "lamb = 0 \n",
    "thi = np.load('./data/ex03.reg.w.npy') #initial parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the algorithm and print the number of elements correctly classified. Also, we show the final weights and a graph of the loss. Running this part could take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Progress : ( 9999 / 10000)   | loss : 68.6395199 out of 200 predictions correct\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQlzCnoRkTCnoBcRCXP/HyAguOnFTWOJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f7a27e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "op_th, ls = train(Xs, ys, thi, lr, T, lamb)\n",
    "gz = eval(Xt, op_th) #evaluating the logistic function\n",
    "y_predict = (gz >= 0.5).astype(np.int32)\n",
    "correct = np.sum(y_predict == yt)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "plt.plot(ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM1$, $\\bf TM2$, $\\bf TM3$, $\\bf TM4$ Train again, but change the learning rate (lr) to 1e-5. Compare the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Progress : ( 9999 / 10000)   | loss : 101.7821199 out of 200 predictions correct\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8SgQREQEUCCIiIhHgSAiIgD8fwP7Asi4UJpxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b472df410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-5 #TODO set the learning rate\n",
    "T = 10000\n",
    "lamb = 0 \n",
    "thi = np.load('./data/ex03.reg.w.npy') #initial parameters.\n",
    "op_th, ls = train(Xs, ys, thi, lr, T, lamb)\n",
    "gz = eval(Xt, op_th) #evaluating the logistic function\n",
    "y_predict = (gz >= 0.5).astype(np.int32)\n",
    "correct = np.sum(y_predict == yt)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "plt.plot(ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this exercise is composed by 2000 images for training and 876 for testing. Each image is a $24\\times 24$ bounding box obtained from a blood sample and can contain either background or a blood parasite know as T. cruzi. The dataset was one of the result from a Mexican project for the diagnosis of Chagas disease (CONACYT/SALUD-2009-C01-113848, contact: Dr. Hugo Ruiz rpina@correo.uady.mx). The picture bellow show some negative (top) and positive (bottom) samples from the dataset. \n",
    "<img src=\"dataset.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are $m\\times n$ numpy arrays and the labels are $m\\times 1$ numpy arrays with $m$ the number of samples and $n$ the number of features. We have $m=2000$ for the training set and $m=876$ for the testing set. The number of features is $n=12$. We save this values for the training set in two variables. As before, we load some libraries and the dataset for training and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you'll need the python module cvxopt. You can install via \"pip install cvxopt\" (For more detailed installations options see http://cvxopt.org/install/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "from data import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import data.svm as svm\n",
    "\n",
    "Xs = np.load('./data/ex03.chagas.ft.train.npy') #training features\n",
    "ys = np.load('./data/ex03.chagas.label.1-1.train.npy') #training labels\n",
    "\n",
    "Xt = np.load('./data/ex03.chagas.ft.test.npy') #testing features\n",
    "yt = np.load('./data/ex03.chagas.label.1-1.test.npy') #testing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see some of the samples and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Features Matrix\n",
      "Shape: (2000, 12)\n",
      "First 5 elements:\n",
      "[[ 0.57777097  0.54774646  0.69579929  0.0460708   0.10126769  0.08421694\n",
      "   0.33381929  0.30002618  0.48413665  0.00212252  0.01025514  0.00709249]\n",
      " [ 0.52168437  0.52414216  0.66214597  0.02717556  0.02799127  0.02858336\n",
      "   0.27215458  0.274725    0.43843728  0.00073851  0.00078351  0.00081701]\n",
      " [ 0.53195125  0.45142293  0.62625272  0.0354268   0.11623582  0.08456892\n",
      "   0.28297214  0.20378266  0.39219247  0.00125506  0.01351077  0.0071519 ]\n",
      " [ 0.56395697  0.48425926  0.68728214  0.03398967  0.11100243  0.09859347\n",
      "   0.31804747  0.23450703  0.47235673  0.0011553   0.01232154  0.00972067]\n",
      " [ 0.52086057  0.48314951  0.62939815  0.0350372   0.09734438  0.07128154\n",
      "   0.27129573  0.23343345  0.39614203  0.00122761  0.00947593  0.00508106]]\n",
      "*Labels\n",
      "Shape: (2000, 1)\n",
      "First 5 elements:\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('*Features Matrix')\n",
    "print('Shape: ' + str(Xs.shape))\n",
    "print('First 5 elements:\\n' + str(Xs[0:5]))\n",
    "print('*Labels')\n",
    "print('Shape: ' +  str(ys.shape))\n",
    "print('First 5 elements:\\n' + str(ys[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the linear kernel as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_kernel(x, z, kernel_params=None):\n",
    "    return np.dot(x, z.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where x and z are $n-$ dimensional numpy arrays (shape=[n,]) corresponding to particular features vectors. The function returns a single float. We can compute the kernel for the 0 and 1 features as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44127970537\n"
     ]
    }
   ],
   "source": [
    "K_x = linear_kernel(Xs[0], Xs[1])\n",
    "print(K_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, following a similar strategy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM2$ Compute the polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_kernel(x, z, kernel_params={'a' : 1, 'c' : 1, 'd' : 3}):\n",
    "    a = kernel_params['a']\n",
    "    c = kernel_params['c']\n",
    "    d = kernel_params['d']\n",
    "    return (a*np.dot(x,z.T) + c)**d\n",
    "    \n",
    "    #TODO Compute the polynomial kernel with the given parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the values for some pair of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.5496525513\n",
      "13.1720336949\n"
     ]
    }
   ],
   "source": [
    "K_x = polynomial_kernel(Xs[0], Xs[1])\n",
    "print(K_x)\n",
    "K_x = polynomial_kernel(Xs[3], Xs[20])\n",
    "print(K_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you should've obtain values around 14.5496 and 13.1720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM3$ Compute the radial basis function (gaussian) kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, z, kernel_params={'sigma' : 5.0}):\n",
    "    sigma = kernel_params['sigma']\n",
    "    return np.exp(-0.5*(1.0/sigma**2)*np.sum((x-z)**2))\n",
    "    #TODO Compute the gaussian kernel with the given parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999593700488\n",
      "0.99983772681\n"
     ]
    }
   ],
   "source": [
    "K_x = gaussian_kernel(Xs[0], Xs[1])\n",
    "print(K_x)\n",
    "K_x = gaussian_kernel(Xs[3], Xs[20])\n",
    "print(K_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get values arround: 0.9996 and 0.9998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this functions, the parameter \"kernel_params\" is a dictionary with the parameters for the kernel. \n",
    "In order to complete the svm code, we need to compute the kernel matrix. Also, once obtained the optimal Lagrange multipliers, we need to obtain the support vectors. (Note: if needed you can compute the norm of a vector with np.linalg.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf TM1}$ Given the features matrix (Xs) and a particular kernel, compute the kernel matrix (Gram matrix).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(X, kernel, kernel_params={'a' : 1, 'c' : 1, 'd' : 3, 'sigma' : 0.5}):\n",
    "    if kernel == linear_kernel:\n",
    "        return linear_kernel(X,X)\n",
    "    elif kernel == polynomial_kernel:\n",
    "        return polynomial_kernel(X,X,kernel_params)\n",
    "    elif kernel == gaussian_kernel:\n",
    "        return gaussian_kernel(X,X, kernel_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should return a $m\\times m$ numpy array. Note that the parameter \"kernel\" takes a function as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check, let's compute the kernel matrix for the first 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5734516   1.44127971  1.3565511 ]\n",
      " [ 1.44127971  1.32942692  1.24039027]\n",
      " [ 1.3565511   1.24039027  1.1765158 ]]\n"
     ]
    }
   ],
   "source": [
    "KM = gram_matrix(X=Xs[0:3], kernel=linear_kernel)\n",
    "print(KM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will  obtain an array similar to: <br>\n",
    "[ [ 1.5734516 &nbsp;  1.44127971 &nbsp; 1.3565511 ]<br>\n",
    " [ 1.44127971 &nbsp; 1.32942692 &nbsp; 1.24039027] <br>\n",
    " [ 1.3565511 &nbsp;  1.24039027 &nbsp; 1.1765158 ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM4$ Given a $m$- dimensional numpy array with the lagrange multipliers (input variable $a$), the features matrix (input variable $X$), the labels array (input variable $y$), and the regularization parameter C (for softmargin svm), obtain the support vectors. (Hint: compare the multipliers with a small number instead of zero, e.g. 1e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vectors(a, X, y, C=None):\n",
    "    sva, sv, svy = None, None, None\n",
    "    #Compute the support vectors here\n",
    "    #W = np.sum(a) - 0.5*np.sum(np.dot(y,y.T)*np.dot(a,a.T)*np.dot(X,X.T))\n",
    "    #W = np.sum(a*y*X)\n",
    "    sva, sv, svy =  a[np.where( a<C)], X[np.where(a<C)],y[np.where(a<C)]\n",
    "    #-------------------------------\n",
    "    return sva, sv, svy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should return 3 numpy arrays: a $k$- dimensional array with the lagrange multipliers for the support vectors, a $k \\times n$ numpy array with the support vectors, and a $k$ dimensional array with the labels for the support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the function with the next code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  30.71    23.234  788.32 ]\n",
      "[[ 0.52168437  0.52414216  0.66214597  0.02717556  0.02799127  0.02858336\n",
      "   0.27215458  0.274725    0.43843728  0.00073851  0.00078351  0.00081701]\n",
      " [ 0.52086057  0.48314951  0.62939815  0.0350372   0.09734438  0.07128154\n",
      "   0.27129573  0.23343345  0.39614203  0.00122761  0.00947593  0.00508106]\n",
      " [ 0.54445806  0.55502451  0.68804466  0.03304641  0.09772327  0.07112868\n",
      "   0.29643458  0.30805221  0.47340546  0.00109207  0.00954984  0.00505929]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "aaux = np.array([1.000e+03, 3.071e+01, 1.4039e+05, 1.00e+03, 23.234, 7.8832e+02, 1.000e+03, 1.232e+03])\n",
    "sa_aux, sv_aux, svy_aux = support_vectors(a=aaux, X=Xs[0:8], y=ys[0:8], C=900)\n",
    "print(sa_aux)\n",
    "print(sv_aux)\n",
    "print(svy_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have something similar to: <br>\n",
    "[  30.71 &nbsp;   23.234 &nbsp; 788.32 ]<br>\n",
    "[ [ 0.52168437 &nbsp; 0.52414216 &nbsp; 0.66214597 &nbsp; 0.02717556  &nbsp;0.02799127 &nbsp; 0.02858336<br>\n",
    "   0.27215458 &nbsp; 0.274725   &nbsp; 0.43843728 &nbsp; 0.00073851 &nbsp; 0.00078351 &nbsp; 0.00081701]<br>\n",
    " [ 0.52086057 &nbsp; 0.48314951 &nbsp; 0.62939815 &nbsp; 0.0350372 &nbsp;  0.09734438 &nbsp; 0.07128154<br>\n",
    "   0.27129573 &nbsp; 0.23343345 &nbsp; 0.39614203 &nbsp; 0.00122761 &nbsp; 0.00947593 &nbsp; 0.00508106]<br>\n",
    " [ 0.54445806 &nbsp; 0.55502451 &nbsp; 0.68804466 &nbsp; 0.03304641 &nbsp; 0.09772327 &nbsp; 0.07112868<br>\n",
    "   0.29643458 &nbsp; 0.30805221 &nbsp; 0.47340546 &nbsp; 0.00109207 &nbsp; 0.00954984 &nbsp; 0.00505929] ]<br>\n",
    "[ [-1.]<br>\n",
    " [ 1.]<br>\n",
    " [ 1.] ]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the algorithm. Again, this part could take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.9984e+03 -4.0528e+03  2e+03  3e-14  2e+00\n",
      " 1: -4.8986e+03 -4.9489e+03  5e+01  3e-14  1e+00\n",
      " 2: -4.8145e+05 -4.8150e+05  5e+01  2e-12  1e+00\n",
      " 3: -4.6373e+09 -4.6373e+09  5e+03  4e-08  1e+00\n",
      " 4: -7.5941e+10 -7.5941e+10  8e+04  2e-06  1e+00\n",
      "Terminated (singular KKT matrix).\n",
      "0 support vectors out of 2000 points\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-feff4de0a929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#The function to compute the support vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#A dictionary with the kernel parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgram_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kishan/New Volume1/kishan/STUDY/TUM/SEM_3/MLMI LAB/exercise03/data/svm.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, gram_matrix, support_vectors, kernel_params)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompt_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msv_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kishan/New Volume1/kishan/STUDY/TUM/SEM_3/MLMI LAB/exercise03/data/svm.pyc\u001b[0m in \u001b[0;36mcompt_b\u001b[0;34m(sv, svy, sva, kernel, kernel_params)\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msva\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msvy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mb\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0msvy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m/=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#Setting up the hyperparameters. \n",
    "C = None\n",
    "kernel_params = {\n",
    "    'a' : 1, \n",
    "    'c' : 1, \n",
    "    'd' : 3, \n",
    "    'sigma' : 0.5\n",
    "}\n",
    "#creating the model\n",
    "model = svm.SVM(kernel=gaussian_kernel, C=C)\n",
    "\n",
    "#finding the optimal margin\n",
    "#The function fit take as arguments:\n",
    "#the X feature matrix. \n",
    "#The y vector of labels. \n",
    "#The function to compute the kernel matrix\n",
    "#The function to compute the support vectors\n",
    "#A dictionary with the kernel parameters\n",
    "\n",
    "model.fit(X=Xs, y=ys, gram_matrix=gram_matrix, support_vectors=support_vectors, kernel_params=kernel_params)\n",
    "\n",
    "y_predict = model.predict(Xt)\n",
    "correct = np.sum(y_predict == yt[:,0])\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM1$, $\\bf TM2$, $\\bf TM3$, $\\bf TM4$ Copy the last code in the next line and try with a value of C = 100 and sigma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0092e+05 -6.7488e+05  6e+05  5e-13  8e-12\n",
      " 1: -1.2990e+05 -2.0280e+05  7e+04  2e-12  7e-12\n",
      " 2: -1.9376e+05 -2.0028e+05  7e+03  6e-13  4e-12\n",
      " 3: -1.9435e+05 -1.9451e+05  2e+02  1e-12  7e-12\n",
      " 4: -1.9440e+05 -1.9440e+05  2e+00  1e-12  8e-12\n",
      " 5: -1.9440e+05 -1.9440e+05  2e-02  2e-12  1e-11\n",
      "Optimal solution found.\n",
      "2000 support vectors out of 2000 points\n",
      "667 out of 876 predictions correct\n"
     ]
    }
   ],
   "source": [
    "#Copy and change hyperparameters here\n",
    "#Setting up the hyperparameters. \n",
    "C = 100\n",
    "kernel_params = {\n",
    "    'a' : 1, \n",
    "    'c' : 1, \n",
    "    'd' : 3, \n",
    "    'sigma' : 1.0\n",
    "}\n",
    "#creating the model\n",
    "model = svm.SVM(kernel=gaussian_kernel, C=C)\n",
    "\n",
    "#finding the optimal margin\n",
    "#The function fit take as arguments:\n",
    "#the X feature matrix. \n",
    "#The y vector of labels. \n",
    "#The function to compute the kernel matrix\n",
    "#The function to compute the support vectors\n",
    "#A dictionary with the kernel parameters\n",
    "\n",
    "model.fit(X=Xs, y=ys, gram_matrix=gram_matrix, support_vectors=support_vectors, kernel_params=kernel_params)\n",
    "\n",
    "y_predict = model.predict(Xt)\n",
    "correct = np.sum(y_predict == yt[:,0])\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf TM1$, $\\bf TM2$, $\\bf TM3$, $\\bf TM4$ Again copy and run the test with the polynomial kernel and C = 100. Keep the parameters a, c, and d to their default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0729e+05 -1.6044e+07  5e+07  1e+00  1e-11\n",
      " 1:  3.4324e+05 -5.5779e+06  8e+06  8e-02  8e-12\n",
      " 2:  1.8755e+05 -1.7660e+06  2e+06  2e-02  6e-12\n",
      " 3:  8.7444e+04 -8.0887e+05  1e+06  8e-03  5e-12\n",
      " 4:  3.4923e+04 -3.9424e+05  4e+05  3e-03  5e-12\n",
      " 5:  1.1596e+04 -2.0036e+05  2e+05  1e-03  5e-12\n",
      " 6:  1.7633e+03 -1.0651e+05  1e+05  5e-04  5e-12\n",
      " 7: -2.0712e+03 -7.4463e+04  7e+04  1e-04  6e-12\n",
      " 8: -2.5776e+03 -6.7242e+04  6e+04  9e-05  5e-12\n",
      " 9: -4.8111e+03 -4.5756e+04  4e+04  5e-05  5e-12\n",
      "10: -6.0293e+03 -3.2387e+04  3e+04  2e-05  5e-12\n",
      "11: -6.7371e+03 -2.8931e+04  2e+04  1e-05  5e-12\n",
      "12: -7.5910e+03 -2.4124e+04  2e+04  7e-06  5e-12\n",
      "13: -7.8689e+03 -2.2762e+04  1e+04  5e-06  5e-12\n",
      "14: -8.3269e+03 -1.9584e+04  1e+04  3e-06  6e-12\n",
      "15: -8.6954e+03 -1.8482e+04  1e+04  2e-06  6e-12\n",
      "16: -9.0228e+03 -1.7049e+04  8e+03  1e-06  6e-12\n",
      "17: -9.5818e+03 -1.5486e+04  6e+03  8e-07  6e-12\n",
      "18: -9.9215e+03 -1.4477e+04  5e+03  5e-07  6e-12\n",
      "19: -1.0033e+04 -1.3937e+04  4e+03  3e-07  6e-12\n",
      "20: -1.0287e+04 -1.3257e+04  3e+03  2e-07  6e-12\n",
      "21: -1.0375e+04 -1.2869e+04  2e+03  1e-07  6e-12\n",
      "22: -1.0629e+04 -1.2360e+04  2e+03  6e-08  6e-12\n",
      "23: -1.0766e+04 -1.2057e+04  1e+03  2e-08  6e-12\n",
      "24: -1.0942e+04 -1.1773e+04  8e+02  1e-08  6e-12\n",
      "25: -1.1034e+04 -1.1609e+04  6e+02  7e-09  7e-12\n",
      "26: -1.1126e+04 -1.1455e+04  3e+02  2e-09  8e-12\n",
      "27: -1.1199e+04 -1.1359e+04  2e+02  8e-10  8e-12\n",
      "28: -1.1235e+04 -1.1306e+04  7e+01  3e-13  9e-12\n",
      "29: -1.1261e+04 -1.1279e+04  2e+01  1e-13  9e-12\n",
      "30: -1.1268e+04 -1.1272e+04  3e+00  5e-13  9e-12\n",
      "31: -1.1270e+04 -1.1270e+04  4e-01  8e-13  9e-12\n",
      "32: -1.1270e+04 -1.1270e+04  2e-02  5e-13  9e-12\n",
      "33: -1.1270e+04 -1.1270e+04  2e-04  5e-13  9e-12\n",
      "Optimal solution found.\n",
      "2000 support vectors out of 2000 points\n",
      "863 out of 876 predictions correct\n"
     ]
    }
   ],
   "source": [
    "#Copy and change hyperparameters here\n",
    "#Setting up the hyperparameters. \n",
    "C = 100\n",
    "kernel_params = {\n",
    "    #'a' : 1, \n",
    "    #'c' : 1, \n",
    "    #'d' : 3, \n",
    "    'sigma' : 0.5\n",
    "}\n",
    "#creating the model\n",
    "model = svm.SVM(kernel=polynomial_kernel, C=C)\n",
    "\n",
    "#finding the optimal margin\n",
    "#The function fit take as arguments:\n",
    "#the X feature matrix. \n",
    "#The y vector of labels. \n",
    "#The function to compute the kernel matrix\n",
    "#The function to compute the support vectors\n",
    "#A dictionary with the kernel parameters\n",
    "\n",
    "model.fit(X=Xs, y=ys, gram_matrix=gram_matrix, support_vectors=support_vectors, kernel_params=kernel_params)\n",
    "\n",
    "y_predict = model.predict(Xt)\n",
    "correct = np.sum(y_predict == yt[:,0])\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you can test with different combinations of hyperparameters to improve the accuracy of the algorithm. The code presented here is a basic implementation of SVM. You can use a more robust library in your projects, e.g. LIBSVM (https://www.csie.ntu.edu.tw/~cjlin/libsvm/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
